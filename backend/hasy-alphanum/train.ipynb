{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Conv2D,Conv3D, MaxPooling2D, MaxPooling3D, Activation, \n",
    "                          Flatten, Dense, Dropout, BatchNormalization)\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.vgg16 import VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total unique labels 61\n",
      "61\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60]\n",
      "(3726, 61)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADPFJREFUeJzt3W+IZfV9x/H3p0bbEoVod5Rl1W4iUiKlWXVYBEtIkxqsFFRoQR+EfSDdUCIopA/EQmuhD0ypSh8Uy1qXLMVqbVVcirRZxCCBYpy167pm22pk22xcdkds0D5pqn774J6FcTN35nr/nDuzv/cLLvfc35y758vZ+czv3PO753dSVUhqz8/NuwBJ82H4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGvWpSd6c5EbgL4BzgL+uqvvXWn/Lli21ffv2STZ5Vjl48GBv27r22mt725bm59ixY7zzzjsZZd2xw5/kHOAvgRuA48DLSfZX1Q+GvWf79u0sLS2Nu8mzTjLS/9FUuN/bsLi4OPK6kxz27wTerKq3quqnwBPAzRP8e5J6NEn4twE/WvH6eNcmaROYJPyrHbP+zCWCSXYnWUqytLy8PMHmJE3TJOE/Dly24vWlwNtnrlRVe6pqsaoWFxYWJticpGmaJPwvA1cm+WyS84DbgP3TKUvSrI19tr+qPkhyJ/DPDIb69lbV61OrTFO11siCE7q0aaJx/qp6DnhuSrVI6pHf8JMaZfilRhl+qVGGX2qU4ZcaNdHZfs3HWkNz41ws5DBgm+z5pUYZfqlRhl9qlOGXGmX4pUZ5tn/G+pyqC4afne+7Dm189vxSowy/1CjDLzXK8EuNMvxSowy/1CiH+jaoaV9QM+7FQF70c/ay55caZfilRhl+qVGGX2qU4ZcaZfilRk001JfkGPA+8CHwQVUtTqOozcYr5rQZTWOc/zeq6p0p/DuSeuRhv9SoScNfwHeSHEyyexoFSerHpIf911fV20kuBg4k+beqenHlCt0fhd0Al19++YSbkzQtE/X8VfV293wKeAbYuco6e6pqsaoWFxYWJtmcpCkaO/xJPp3kgtPLwFeBI9MqTNJsTXLYfwnwTDfM9Sngb6vqn6ZSVSM2ylVx0779lzaHscNfVW8BX5hiLZJ65FCf1CjDLzXK8EuNMvxSowy/1Cgn8NTYnNxzc7Pnlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapQX9szYZr/Axfn9zl72/FKjDL/UKMMvNcrwS40y/FKjDL/UqHXDn2RvklNJjqxouyjJgSRvdM8XzrbMjSHJqg/9LPfVxjdKz/9t4MYz2u4Bnq+qK4Hnu9eSNpF1w19VLwLvntF8M7CvW94H3DLluiTN2Lif+S+pqhMA3fPF0ytJUh9mfsIvye4kS0mWlpeXZ705SSMaN/wnk2wF6J5PDVuxqvZU1WJVLS4sLIy5OUnTNm749wO7uuVdwLPTKUdSX0YZ6nsc+BfgV5IcT3IHcD9wQ5I3gBu619K6hg0BOgzYv3Uv6a2q24f86CtTrkVSj/yGn9Qowy81yvBLjTL8UqMMv9QoJ/DU2Jzcc3Oz55caZfilRhl+qVGGX2qU4ZcaZfilRjnUpw1jreHBzX7Pw43Inl9qlOGXGmX4pUYZfqlRhl9qlGf7NRPDzs6Pe8HPuO9zlGA4e36pUYZfapThlxpl+KVGGX6pUYZfatS6Q31J9gK/DZyqql/t2u4Dfg84fdvde6vquVkVuZk5l502qlF6/m8DN67S/lBV7egeBl/aZNYNf1W9CLzbQy2SejTJZ/47kxxOsjfJhVOrSFIvxg3/w8AVwA7gBPDAsBWT7E6ylGRpeXl52GqSejZW+KvqZFV9WFUfAY8AO9dYd09VLVbV4sLCwrh1SpqyscKfZOuKl7cCR6ZTjqS+jDLU9zjwJWBLkuPAHwNfSrIDKOAY8PUZ1iitySv3xrNu+Kvq9lWaH51BLZJ65Df8pEYZfqlRhl9qlOGXGmX4pUY5gecG1eLw1Swm92xxP47Knl9qlOGXGmX4pUYZfqlRhl9qlOGXGuVQ34w51DS6tfaVE6FOnz2/1CjDLzXK8EuNMvxSowy/1CjP9k+BZ/Q3rmGjBP6f2fNLzTL8UqMMv9Qowy81yvBLjTL8UqPWDX+Sy5K8kORokteT3NW1X5TkQJI3uuez4jbdSYY+ND9VNfQxDv+fR+v5PwC+WVWfB64DvpHkKuAe4PmquhJ4vnstaZNYN/xVdaKqXumW3weOAtuAm4F93Wr7gFtmVaSk6ftEn/mTbAeuBl4CLqmqEzD4AwFcPO3iJM3OyOFPcj7wFHB3Vb33Cd63O8lSkqXl5eVxapQ0AyOFP8m5DIL/WFU93TWfTLK1+/lW4NRq762qPVW1WFWLCwsL06hZ0hSMcrY/wKPA0ap6cMWP9gO7uuVdwLPTL0/SrIxyVd/1wNeA15Ic6truBe4HnkxyB/BfwO/OpkRJs7Bu+Kvqe8Cwwc+vTLccSX3xG35Sowy/1CjDLzXK8EuNMvxSo5qcwLOlK7da4G2+xmPPLzXK8EuNMvxSowy/1CjDLzXK8EuNanKob1ze360Naw0Pnk2/A/b8UqMMv9Qowy81yvBLjTL8UqM826+z2rCz8+Ne8HM2jQTY80uNMvxSowy/1CjDLzXK8EuNMvxSo0a5V99lSV5IcjTJ60nu6trvS/LjJIe6x02zL3d0SYY+JI02zv8B8M2qeiXJBcDBJAe6nz1UVX8+u/Ikzcoo9+o7AZzolt9PchTYNuvCJM3WJ/rMn2Q7cDXwUtd0Z5LDSfYmuXDKtUmaoZHDn+R84Cng7qp6D3gYuALYweDI4IEh79udZCnJ0vLy8hRKljQNI4U/ybkMgv9YVT0NUFUnq+rDqvoIeATYudp7q2pPVS1W1eLCwsK06pY0oVHO9gd4FDhaVQ+uaN+6YrVbgSPTL0/SrIxytv964GvAa0kOdW33Arcn2QEUcAz4+kwq7NlmuzJL4/EWX6Od7f8esNreeG765Ujqi9/wkxpl+KVGGX6pUYZfapThlxq1qSfwbGVIRpoFe36pUYZfapThlxpl+KVGGX6pUYZfatSmHuob98osr9zTLGy23zl7fqlRhl9qlOGXGmX4pUYZfqlRhl9q1KYe6lvLRhxa0ebQyuSe9vxSowy/1CjDLzXK8EuNMvxSo0a5V98vJPl+kleTvJ7kT7r2zyZ5KckbSf4uyXmzL1fanJIMfczLKD3//wJfrqovMLgd941JrgO+BTxUVVcC/w3cMbsyJU3buuGvgf/pXp7bPQr4MvAPXfs+4JaZVChpJkb6zJ/knO4OvaeAA8APgZ9U1QfdKseBbbMpUdIsjBT+qvqwqnYAlwI7gc+vttpq702yO8lSkqXl5eXxK5U0VZ/obH9V/QT4LnAd8Jkkp78efCnw9pD37KmqxapaXFhYmKRWSVM0ytn+hSSf6ZZ/EfhN4CjwAvA73Wq7gGdnVaSk6Rvlwp6twL4k5zD4Y/FkVf1jkh8ATyT5U+BfgUdnWKekKVs3/FV1GLh6lfa3GHz+l7QJ+Q0/qVGGX2qU4ZcaZfilRhl+qVHpc667JMvAf3YvtwDv9Lbx4azj46zj4zZbHb9cVSN9m67X8H9sw8lSVS3OZePWYR3W4WG/1CrDLzVqnuHfM8dtr2QdH2cdH3fW1jG3z/yS5svDfqlRcwl/khuT/HuSN5PcM48aujqOJXktyaEkSz1ud2+SU0mOrGi7KMmBbkLUA0kunFMd9yX5cbdPDiW5qYc6LkvyQpKj3SSxd3Xtve6TNerodZ/0NmluVfX6AM5hMA3Y54DzgFeBq/quo6vlGLBlDtv9InANcGRF258B93TL9wDfmlMd9wF/0PP+2Apc0y1fAPwHcFXf+2SNOnrdJ0CA87vlc4GXGEyg8yRwW9f+V8DvT7KdefT8O4E3q+qtqvop8ARw8xzqmJuqehF494zmmxlMhAo9TYg6pI7eVdWJqnqlW36fwWQx2+h5n6xRR69qYOaT5s4j/NuAH614Pc/JPwv4TpKDSXbPqYbTLqmqEzD4JQQunmMtdyY53H0smPnHj5WSbGcwf8RLzHGfnFEH9LxP+pg0dx7hX+0uBfMacri+qq4Bfgv4RpIvzqmOjeRh4AoG92g4ATzQ14aTnA88BdxdVe/1td0R6uh9n9QEk+aOah7hPw5ctuL10Mk/Z62q3u6eTwHPMN+ZiU4m2QrQPZ+aRxFVdbL7xfsIeISe9kmScxkE7rGqerpr7n2frFbHvPZJt+1PPGnuqOYR/peBK7szl+cBtwH7+y4iyaeTXHB6GfgqcGTtd83UfgYTocIcJ0Q9HbbOrfSwTzK4Z9WjwNGqenDFj3rdJ8Pq6Huf9DZpbl9nMM84m3kTgzOpPwT+cE41fI7BSMOrwOt91gE8zuDw8f8YHAndAfwS8DzwRvd80Zzq+BvgNeAwg/Bt7aGOX2dwCHsYONQ9bup7n6xRR6/7BPg1BpPiHmbwh+aPVvzOfh94E/h74Ocn2Y7f8JMa5Tf8pEYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGvX/JIaUdyN+p2oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_path = './alpha_numeric_dataset/alphanum-hasy-data-X.npy'\n",
    "y_path = './alpha_numeric_dataset/alphanum-hasy-data-Y.npy'\n",
    "x = np.load(x_path)\n",
    "y = np.load(y_path)\n",
    "print('total unique labels', len(np.unique(y)))\n",
    "x_og = x\n",
    "num_classes = len(np.unique(y))\n",
    "# convert labels to be zero-based\n",
    "for i in range(31, 57):\n",
    "    y[y == i] -= 31\n",
    "for i in range(70, 80):\n",
    "    y[y == i] -= 44\n",
    "for i in range(90, 109):\n",
    "    y[y == i] -= 54\n",
    "for i in range(110, 116):\n",
    "    y[y == i] -= 55\n",
    "    \n",
    "x = x.reshape(x.shape[0], 32, 32 , 1)\n",
    "x = np.squeeze(np.stack((x,) * 3, -1)) # convert to RGB\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n",
    "                                                     random_state=0)\n",
    "\n",
    "plt.imshow(x[3], interpolation='nearest')\n",
    "print(len(np.unique(y_train)))\n",
    "print(np.unique(y_train))\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_train/=255\n",
    "x_test/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save original images for reference\n",
    "# for img, name in zip(x_og, y):\n",
    "#     plt.imsave(f'./hasy-alphanum/img/{name}', img) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range=10)\n",
    "train_batch = gen.flow(x_train, y_train, batch_size=128)\n",
    "test_batch = gen.flow(x_test, y_test, batch_size=128)\n",
    "# train_batch = ImageDataGenerator.flow(X, y_train, batch_size=64)\n",
    "# single_img = next(train_batch)\n",
    "# print(single_img[0].shape)\n",
    "# print(single_img[1][0].shape)\n",
    "# print(np.where(single_img[1][0] == 1)[0])\n",
    "# single_img = single_img[0].reshape(single_img[0].shape[0], 32, 32)\n",
    "# print(single_img.shape)\n",
    "# plt.imshow(single_img[0])\n",
    "# print(len(x_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(32, 32, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 61)                7869      \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 61)                0         \n",
      "=================================================================\n",
      "Total params: 530,941\n",
      "Trainable params: 529,469\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "print(x_train.ndim)\n",
    "print(x_train.shape[1:])\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "# Fully connected layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(61))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "58/58 [==============================] - 15s 252ms/step - loss: 1.8681 - acc: 0.4252 - val_loss: 1.2815 - val_acc: 0.5690\n",
      "Epoch 2/150\n",
      "58/58 [==============================] - 16s 268ms/step - loss: 1.6799 - acc: 0.4678 - val_loss: 1.1630 - val_acc: 0.6024\n",
      "Epoch 3/150\n",
      "58/58 [==============================] - 16s 277ms/step - loss: 1.5780 - acc: 0.5049 - val_loss: 1.0423 - val_acc: 0.6244\n",
      "Epoch 4/150\n",
      "58/58 [==============================] - 17s 290ms/step - loss: 1.5011 - acc: 0.5201 - val_loss: 0.9627 - val_acc: 0.6810\n",
      "Epoch 5/150\n",
      "58/58 [==============================] - 17s 296ms/step - loss: 1.3914 - acc: 0.5404 - val_loss: 0.9181 - val_acc: 0.6567\n",
      "Epoch 6/150\n",
      "58/58 [==============================] - 16s 273ms/step - loss: 1.3392 - acc: 0.5567 - val_loss: 0.9621 - val_acc: 0.6459\n",
      "Epoch 7/150\n",
      "58/58 [==============================] - 17s 285ms/step - loss: 1.2613 - acc: 0.5763 - val_loss: 0.8618 - val_acc: 0.6922\n",
      "Epoch 8/150\n",
      "58/58 [==============================] - 17s 294ms/step - loss: 1.1634 - acc: 0.6016 - val_loss: 1.0918 - val_acc: 0.6294\n",
      "Epoch 9/150\n",
      "58/58 [==============================] - 16s 277ms/step - loss: 1.1968 - acc: 0.6013 - val_loss: 1.0010 - val_acc: 0.6530\n",
      "Epoch 10/150\n",
      "58/58 [==============================] - 17s 287ms/step - loss: 1.1290 - acc: 0.6071 - val_loss: 1.0711 - val_acc: 0.6482\n",
      "Epoch 11/150\n",
      "58/58 [==============================] - 16s 275ms/step - loss: 1.0991 - acc: 0.6224 - val_loss: 0.7363 - val_acc: 0.7177\n",
      "Epoch 12/150\n",
      "58/58 [==============================] - 16s 273ms/step - loss: 1.0564 - acc: 0.6372 - val_loss: 0.7111 - val_acc: 0.7345\n",
      "Epoch 13/150\n",
      "58/58 [==============================] - 17s 291ms/step - loss: 1.0059 - acc: 0.6454 - val_loss: 0.7685 - val_acc: 0.7102\n",
      "Epoch 14/150\n",
      "58/58 [==============================] - 16s 279ms/step - loss: 0.9625 - acc: 0.6637 - val_loss: 0.7048 - val_acc: 0.7365\n",
      "Epoch 15/150\n",
      "58/58 [==============================] - 16s 272ms/step - loss: 0.9804 - acc: 0.6590 - val_loss: 0.7177 - val_acc: 0.7264\n",
      "Epoch 16/150\n",
      "58/58 [==============================] - 16s 280ms/step - loss: 0.9577 - acc: 0.6709 - val_loss: 0.7097 - val_acc: 0.7183\n",
      "Epoch 17/150\n",
      "58/58 [==============================] - 16s 277ms/step - loss: 0.8912 - acc: 0.6858 - val_loss: 0.6117 - val_acc: 0.7799\n",
      "Epoch 18/150\n",
      "58/58 [==============================] - 16s 274ms/step - loss: 0.9163 - acc: 0.6704 - val_loss: 0.6266 - val_acc: 0.7559\n",
      "Epoch 19/150\n",
      "58/58 [==============================] - 16s 277ms/step - loss: 0.9343 - acc: 0.6788 - val_loss: 0.6317 - val_acc: 0.7506\n",
      "Epoch 20/150\n",
      "58/58 [==============================] - 16s 277ms/step - loss: 0.8465 - acc: 0.6925 - val_loss: 0.6716 - val_acc: 0.7326\n",
      "Epoch 21/150\n",
      "58/58 [==============================] - 16s 278ms/step - loss: 0.8211 - acc: 0.7005 - val_loss: 0.6410 - val_acc: 0.7662\n",
      "Epoch 22/150\n",
      "58/58 [==============================] - 17s 292ms/step - loss: 0.8052 - acc: 0.7078 - val_loss: 0.6990 - val_acc: 0.7300\n",
      "Epoch 23/150\n",
      "58/58 [==============================] - 16s 276ms/step - loss: 0.8509 - acc: 0.6926 - val_loss: 0.8229 - val_acc: 0.7058\n",
      "Epoch 24/150\n",
      "58/58 [==============================] - 16s 280ms/step - loss: 0.7955 - acc: 0.7050 - val_loss: 0.5976 - val_acc: 0.7568\n",
      "Epoch 25/150\n",
      "58/58 [==============================] - 18s 302ms/step - loss: 0.7903 - acc: 0.7128 - val_loss: 0.6681 - val_acc: 0.7680\n",
      "Epoch 26/150\n",
      "58/58 [==============================] - 16s 274ms/step - loss: 0.7263 - acc: 0.7244 - val_loss: 0.6624 - val_acc: 0.7212\n",
      "Epoch 27/150\n",
      "58/58 [==============================] - 15s 266ms/step - loss: 0.7515 - acc: 0.7202 - val_loss: 0.5950 - val_acc: 0.7687\n",
      "Epoch 28/150\n",
      "58/58 [==============================] - 16s 281ms/step - loss: 0.7295 - acc: 0.7241 - val_loss: 0.7476 - val_acc: 0.7301\n",
      "Epoch 29/150\n",
      "58/58 [==============================] - 16s 281ms/step - loss: 0.7309 - acc: 0.7328 - val_loss: 0.6482 - val_acc: 0.7500\n",
      "Epoch 30/150\n",
      "58/58 [==============================] - 16s 273ms/step - loss: 0.7306 - acc: 0.7326 - val_loss: 0.9289 - val_acc: 0.6941\n",
      "Epoch 31/150\n",
      "58/58 [==============================] - 15s 266ms/step - loss: 0.7064 - acc: 0.7302 - val_loss: 0.6204 - val_acc: 0.7612\n",
      "Epoch 32/150\n",
      "58/58 [==============================] - 16s 271ms/step - loss: 0.7088 - acc: 0.7433 - val_loss: 0.7060 - val_acc: 0.7400\n",
      "Epoch 33/150\n",
      "58/58 [==============================] - 16s 278ms/step - loss: 0.6876 - acc: 0.7314 - val_loss: 0.5769 - val_acc: 0.7637\n",
      "Epoch 34/150\n",
      "58/58 [==============================] - 16s 269ms/step - loss: 0.7008 - acc: 0.7367 - val_loss: 0.6003 - val_acc: 0.7800\n",
      "Epoch 35/150\n",
      "58/58 [==============================] - 15s 259ms/step - loss: 0.6919 - acc: 0.7427 - val_loss: 0.5763 - val_acc: 0.7711\n",
      "Epoch 36/150\n",
      "58/58 [==============================] - 15s 262ms/step - loss: 0.6787 - acc: 0.7419 - val_loss: 0.6746 - val_acc: 0.7456\n",
      "Epoch 37/150\n",
      "58/58 [==============================] - 16s 272ms/step - loss: 0.6704 - acc: 0.7499 - val_loss: 0.5816 - val_acc: 0.7699\n",
      "Epoch 38/150\n",
      "58/58 [==============================] - 16s 281ms/step - loss: 0.6880 - acc: 0.7437 - val_loss: 0.5978 - val_acc: 0.7665\n",
      "Epoch 39/150\n",
      "58/58 [==============================] - 17s 285ms/step - loss: 0.6488 - acc: 0.7513 - val_loss: 0.5972 - val_acc: 0.7668\n",
      "Epoch 40/150\n",
      "58/58 [==============================] - 17s 289ms/step - loss: 0.6488 - acc: 0.7559 - val_loss: 0.6805 - val_acc: 0.7226\n",
      "Epoch 41/150\n",
      "58/58 [==============================] - 16s 283ms/step - loss: 0.6378 - acc: 0.7532 - val_loss: 0.5468 - val_acc: 0.7755\n",
      "Epoch 42/150\n",
      "58/58 [==============================] - 17s 290ms/step - loss: 0.6387 - acc: 0.7585 - val_loss: 0.6018 - val_acc: 0.7812\n",
      "Epoch 43/150\n",
      "58/58 [==============================] - 16s 284ms/step - loss: 0.6093 - acc: 0.7639 - val_loss: 0.7489 - val_acc: 0.7295\n",
      "Epoch 44/150\n",
      "58/58 [==============================] - 17s 297ms/step - loss: 0.6040 - acc: 0.7614 - val_loss: 0.5432 - val_acc: 0.7830\n",
      "Epoch 45/150\n",
      "58/58 [==============================] - 16s 279ms/step - loss: 0.6283 - acc: 0.7643 - val_loss: 0.5680 - val_acc: 0.7755\n",
      "Epoch 46/150\n",
      "58/58 [==============================] - 16s 271ms/step - loss: 0.5910 - acc: 0.7628 - val_loss: 0.6455 - val_acc: 0.7882\n",
      "Epoch 47/150\n",
      "58/58 [==============================] - 16s 268ms/step - loss: 0.5767 - acc: 0.7854 - val_loss: 0.6006 - val_acc: 0.7761\n",
      "Epoch 48/150\n",
      "58/58 [==============================] - 16s 278ms/step - loss: 0.5832 - acc: 0.7668 - val_loss: 0.5987 - val_acc: 0.7904\n",
      "Epoch 49/150\n",
      "58/58 [==============================] - 16s 275ms/step - loss: 0.6277 - acc: 0.7577 - val_loss: 0.6088 - val_acc: 0.7674\n",
      "Epoch 50/150\n",
      "58/58 [==============================] - 16s 275ms/step - loss: 0.5774 - acc: 0.7759 - val_loss: 0.5193 - val_acc: 0.7847\n",
      "Epoch 51/150\n",
      "58/58 [==============================] - 16s 274ms/step - loss: 0.5811 - acc: 0.7695 - val_loss: 0.5694 - val_acc: 0.7867\n",
      "Epoch 52/150\n",
      "58/58 [==============================] - 16s 274ms/step - loss: 0.5895 - acc: 0.7635 - val_loss: 0.5493 - val_acc: 0.7848\n",
      "Epoch 53/150\n",
      "58/58 [==============================] - 16s 275ms/step - loss: 0.6013 - acc: 0.7650 - val_loss: 0.5461 - val_acc: 0.7687\n",
      "Epoch 54/150\n",
      "58/58 [==============================] - 16s 274ms/step - loss: 0.5783 - acc: 0.7814 - val_loss: 0.6214 - val_acc: 0.7735\n",
      "Epoch 55/150\n",
      "58/58 [==============================] - 16s 269ms/step - loss: 0.5668 - acc: 0.7788 - val_loss: 0.5278 - val_acc: 0.7935\n",
      "Epoch 56/150\n",
      "58/58 [==============================] - 16s 273ms/step - loss: 0.5636 - acc: 0.7811 - val_loss: 0.6247 - val_acc: 0.7767\n",
      "Epoch 57/150\n",
      "58/58 [==============================] - 16s 275ms/step - loss: 0.5814 - acc: 0.7741 - val_loss: 0.6313 - val_acc: 0.7631\n",
      "Epoch 58/150\n",
      "58/58 [==============================] - 16s 275ms/step - loss: 0.5962 - acc: 0.7695 - val_loss: 0.6070 - val_acc: 0.7753\n",
      "Epoch 59/150\n",
      "58/58 [==============================] - 16s 280ms/step - loss: 0.5721 - acc: 0.7804 - val_loss: 0.5480 - val_acc: 0.7935\n",
      "Epoch 60/150\n",
      "58/58 [==============================] - 16s 280ms/step - loss: 0.5350 - acc: 0.7841 - val_loss: 0.5592 - val_acc: 0.7836\n",
      "Epoch 61/150\n",
      "58/58 [==============================] - 16s 276ms/step - loss: 0.5576 - acc: 0.7868 - val_loss: 0.5942 - val_acc: 0.7631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/150\n",
      "58/58 [==============================] - 16s 278ms/step - loss: 0.5424 - acc: 0.7845 - val_loss: 0.5617 - val_acc: 0.7771\n",
      "Epoch 63/150\n",
      "58/58 [==============================] - 16s 277ms/step - loss: 0.5169 - acc: 0.7938 - val_loss: 0.5920 - val_acc: 0.7836\n",
      "Epoch 64/150\n",
      "58/58 [==============================] - 17s 284ms/step - loss: 0.5231 - acc: 0.7899 - val_loss: 0.5467 - val_acc: 0.7817\n",
      "Epoch 65/150\n",
      "58/58 [==============================] - 16s 281ms/step - loss: 0.5394 - acc: 0.7856 - val_loss: 0.5417 - val_acc: 0.7923\n",
      "Epoch 66/150\n",
      "58/58 [==============================] - 16s 281ms/step - loss: 0.5437 - acc: 0.7799 - val_loss: 0.5800 - val_acc: 0.7706\n",
      "Epoch 67/150\n",
      "58/58 [==============================] - 16s 282ms/step - loss: 0.5192 - acc: 0.7902 - val_loss: 0.5709 - val_acc: 0.8004\n",
      "Epoch 68/150\n",
      "58/58 [==============================] - 16s 284ms/step - loss: 0.5401 - acc: 0.7908 - val_loss: 0.5703 - val_acc: 0.7631\n",
      "Epoch 69/150\n",
      "58/58 [==============================] - 16s 282ms/step - loss: 0.5090 - acc: 0.7945 - val_loss: 0.5292 - val_acc: 0.8041\n",
      "Epoch 70/150\n",
      "58/58 [==============================] - 17s 286ms/step - loss: 0.5122 - acc: 0.7965 - val_loss: 0.8851 - val_acc: 0.7265\n",
      "Epoch 71/150\n",
      "58/58 [==============================] - 16s 282ms/step - loss: 0.5232 - acc: 0.7863 - val_loss: 0.5635 - val_acc: 0.7886\n",
      "Epoch 72/150\n",
      "58/58 [==============================] - 16s 276ms/step - loss: 0.5151 - acc: 0.7976 - val_loss: 0.5773 - val_acc: 0.7973\n",
      "Epoch 73/150\n",
      "58/58 [==============================] - 17s 301ms/step - loss: 0.5073 - acc: 0.7946 - val_loss: 0.5555 - val_acc: 0.8085\n",
      "Epoch 74/150\n",
      "58/58 [==============================] - 16s 279ms/step - loss: 0.4930 - acc: 0.8003 - val_loss: 0.5685 - val_acc: 0.7906\n",
      "Epoch 75/150\n",
      "58/58 [==============================] - 16s 278ms/step - loss: 0.5055 - acc: 0.7982 - val_loss: 0.5223 - val_acc: 0.7917\n",
      "Epoch 76/150\n",
      "58/58 [==============================] - 16s 273ms/step - loss: 0.4867 - acc: 0.7970 - val_loss: 0.5363 - val_acc: 0.7954\n",
      "Epoch 77/150\n",
      "58/58 [==============================] - 17s 300ms/step - loss: 0.4899 - acc: 0.8023 - val_loss: 0.5607 - val_acc: 0.7960\n",
      "Epoch 78/150\n",
      "58/58 [==============================] - 16s 280ms/step - loss: 0.4883 - acc: 0.8096 - val_loss: 0.8941 - val_acc: 0.7053\n",
      "Epoch 79/150\n",
      "58/58 [==============================] - 15s 264ms/step - loss: 0.4950 - acc: 0.8066 - val_loss: 0.5704 - val_acc: 0.7792\n",
      "Epoch 80/150\n",
      "58/58 [==============================] - 15s 253ms/step - loss: 0.4884 - acc: 0.8066 - val_loss: 0.5472 - val_acc: 0.7991\n",
      "Epoch 81/150\n",
      "58/58 [==============================] - 15s 267ms/step - loss: 0.4868 - acc: 0.8084 - val_loss: 0.5913 - val_acc: 0.7780\n",
      "Epoch 82/150\n",
      "58/58 [==============================] - 14s 247ms/step - loss: 0.4640 - acc: 0.8125 - val_loss: 0.6444 - val_acc: 0.7682\n",
      "Epoch 83/150\n",
      "58/58 [==============================] - 16s 270ms/step - loss: 0.4632 - acc: 0.8101 - val_loss: 0.6010 - val_acc: 0.8029\n",
      "Epoch 84/150\n",
      "58/58 [==============================] - 16s 270ms/step - loss: 0.4965 - acc: 0.8038 - val_loss: 0.6709 - val_acc: 0.7687\n",
      "Epoch 85/150\n",
      "58/58 [==============================] - 16s 269ms/step - loss: 0.5049 - acc: 0.7968 - val_loss: 0.5780 - val_acc: 0.7718\n",
      "Epoch 86/150\n",
      "58/58 [==============================] - 16s 283ms/step - loss: 0.4849 - acc: 0.8039 - val_loss: 0.5315 - val_acc: 0.7941\n",
      "Epoch 87/150\n",
      "58/58 [==============================] - 18s 307ms/step - loss: 0.4785 - acc: 0.8016 - val_loss: 0.6777 - val_acc: 0.7494\n",
      "Epoch 88/150\n",
      "58/58 [==============================] - 16s 275ms/step - loss: 0.4973 - acc: 0.8061 - val_loss: 0.5646 - val_acc: 0.7817\n",
      "Epoch 89/150\n",
      "58/58 [==============================] - 17s 293ms/step - loss: 0.4661 - acc: 0.8077 - val_loss: 0.5203 - val_acc: 0.7923\n",
      "Epoch 90/150\n",
      "58/58 [==============================] - 16s 275ms/step - loss: 0.4828 - acc: 0.8026 - val_loss: 0.5459 - val_acc: 0.7953\n",
      "Epoch 91/150\n",
      "58/58 [==============================] - 16s 269ms/step - loss: 0.4648 - acc: 0.8108 - val_loss: 0.6175 - val_acc: 0.8010\n",
      "Epoch 92/150\n",
      "58/58 [==============================] - 16s 274ms/step - loss: 0.4532 - acc: 0.8177 - val_loss: 0.5793 - val_acc: 0.7998\n",
      "Epoch 93/150\n",
      "58/58 [==============================] - 16s 279ms/step - loss: 0.4196 - acc: 0.8280 - val_loss: 0.5505 - val_acc: 0.8041\n",
      "Epoch 94/150\n",
      "58/58 [==============================] - 16s 275ms/step - loss: 0.4303 - acc: 0.8208 - val_loss: 0.5407 - val_acc: 0.7894\n",
      "Epoch 95/150\n",
      "58/58 [==============================] - 16s 276ms/step - loss: 0.4482 - acc: 0.8136 - val_loss: 0.5707 - val_acc: 0.7998\n",
      "Epoch 96/150\n",
      "58/58 [==============================] - 16s 277ms/step - loss: 0.4318 - acc: 0.8245 - val_loss: 0.6017 - val_acc: 0.7811\n",
      "Epoch 97/150\n",
      "58/58 [==============================] - 16s 283ms/step - loss: 0.4555 - acc: 0.8146 - val_loss: 0.6074 - val_acc: 0.7861\n",
      "Epoch 98/150\n",
      "58/58 [==============================] - 16s 276ms/step - loss: 0.4381 - acc: 0.8194 - val_loss: 1.0627 - val_acc: 0.7000\n",
      "Epoch 99/150\n",
      "58/58 [==============================] - 16s 274ms/step - loss: 0.4226 - acc: 0.8244 - val_loss: 0.6057 - val_acc: 0.7792\n",
      "Epoch 100/150\n",
      "58/58 [==============================] - 16s 281ms/step - loss: 0.4456 - acc: 0.8179 - val_loss: 0.5558 - val_acc: 0.8066\n",
      "Epoch 101/150\n",
      "58/58 [==============================] - 16s 277ms/step - loss: 0.4462 - acc: 0.8182 - val_loss: 0.5241 - val_acc: 0.8035\n",
      "Epoch 102/150\n",
      "58/58 [==============================] - 16s 279ms/step - loss: 0.4452 - acc: 0.8160 - val_loss: 0.5937 - val_acc: 0.7994\n",
      "Epoch 103/150\n",
      "58/58 [==============================] - 16s 279ms/step - loss: 0.4222 - acc: 0.8288 - val_loss: 0.6312 - val_acc: 0.7842\n",
      "Epoch 104/150\n",
      "58/58 [==============================] - 16s 284ms/step - loss: 0.4445 - acc: 0.8224 - val_loss: 0.5691 - val_acc: 0.7917\n",
      "Epoch 105/150\n",
      "58/58 [==============================] - 16s 279ms/step - loss: 0.4159 - acc: 0.8322 - val_loss: 0.5593 - val_acc: 0.8060\n",
      "Epoch 106/150\n",
      "58/58 [==============================] - 16s 280ms/step - loss: 0.4290 - acc: 0.8290 - val_loss: 0.5072 - val_acc: 0.8176\n",
      "Epoch 107/150\n",
      "58/58 [==============================] - 16s 280ms/step - loss: 0.4186 - acc: 0.8245 - val_loss: 0.5750 - val_acc: 0.8016\n",
      "Epoch 108/150\n",
      "58/58 [==============================] - 17s 289ms/step - loss: 0.4448 - acc: 0.8163 - val_loss: 0.5441 - val_acc: 0.7867\n",
      "Epoch 109/150\n",
      "58/58 [==============================] - 17s 285ms/step - loss: 0.4524 - acc: 0.8162 - val_loss: 0.5652 - val_acc: 0.7935\n",
      "Epoch 110/150\n",
      "58/58 [==============================] - 17s 285ms/step - loss: 0.4602 - acc: 0.8119 - val_loss: 0.5711 - val_acc: 0.7918\n",
      "Epoch 111/150\n",
      "58/58 [==============================] - 16s 284ms/step - loss: 0.4323 - acc: 0.8233 - val_loss: 0.5930 - val_acc: 0.8010\n",
      "Epoch 112/150\n",
      "58/58 [==============================] - 16s 283ms/step - loss: 0.4262 - acc: 0.8255 - val_loss: 0.5411 - val_acc: 0.8072\n",
      "Epoch 113/150\n",
      "58/58 [==============================] - 16s 284ms/step - loss: 0.4017 - acc: 0.8404 - val_loss: 0.5015 - val_acc: 0.8128\n",
      "Epoch 114/150\n",
      "58/58 [==============================] - 16s 284ms/step - loss: 0.4271 - acc: 0.8259 - val_loss: 0.6118 - val_acc: 0.7547\n",
      "Epoch 115/150\n",
      "58/58 [==============================] - 16s 280ms/step - loss: 0.5005 - acc: 0.8106 - val_loss: 0.7497 - val_acc: 0.7606\n",
      "Epoch 116/150\n",
      "58/58 [==============================] - 17s 288ms/step - loss: 0.4492 - acc: 0.8177 - val_loss: 0.5385 - val_acc: 0.7960\n",
      "Epoch 117/150\n",
      "58/58 [==============================] - 16s 284ms/step - loss: 0.4352 - acc: 0.8292 - val_loss: 0.5897 - val_acc: 0.7966\n",
      "Epoch 118/150\n",
      "58/58 [==============================] - 16s 275ms/step - loss: 0.4000 - acc: 0.8283 - val_loss: 0.5981 - val_acc: 0.7959\n",
      "Epoch 119/150\n",
      "58/58 [==============================] - 15s 260ms/step - loss: 0.3990 - acc: 0.8332 - val_loss: 0.5541 - val_acc: 0.8097\n",
      "Epoch 120/150\n",
      "58/58 [==============================] - 17s 299ms/step - loss: 0.4062 - acc: 0.8322 - val_loss: 0.6368 - val_acc: 0.7811\n",
      "Epoch 121/150\n",
      "58/58 [==============================] - 16s 273ms/step - loss: 0.4259 - acc: 0.8268 - val_loss: 0.6765 - val_acc: 0.7749\n",
      "Epoch 122/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 16s 271ms/step - loss: 0.3944 - acc: 0.8327 - val_loss: 0.5585 - val_acc: 0.8053\n",
      "Epoch 123/150\n",
      "58/58 [==============================] - 15s 259ms/step - loss: 0.4181 - acc: 0.8275 - val_loss: 0.5519 - val_acc: 0.8072\n",
      "Epoch 124/150\n",
      "58/58 [==============================] - 15s 258ms/step - loss: 0.3976 - acc: 0.8330 - val_loss: 0.5564 - val_acc: 0.8085\n",
      "Epoch 125/150\n",
      "58/58 [==============================] - 15s 257ms/step - loss: 0.3831 - acc: 0.8357 - val_loss: 0.5270 - val_acc: 0.8109\n",
      "Epoch 126/150\n",
      "58/58 [==============================] - 15s 260ms/step - loss: 0.4162 - acc: 0.8283 - val_loss: 0.5682 - val_acc: 0.8141\n",
      "Epoch 127/150\n",
      "58/58 [==============================] - 15s 267ms/step - loss: 0.4020 - acc: 0.8329 - val_loss: 0.6193 - val_acc: 0.7935\n",
      "Epoch 128/150\n",
      "58/58 [==============================] - 16s 271ms/step - loss: 0.4124 - acc: 0.8317 - val_loss: 0.5982 - val_acc: 0.7749\n",
      "Epoch 129/150\n",
      "58/58 [==============================] - 16s 270ms/step - loss: 0.4071 - acc: 0.8307 - val_loss: 0.5413 - val_acc: 0.8091\n",
      "Epoch 130/150\n",
      "58/58 [==============================] - 16s 272ms/step - loss: 0.3953 - acc: 0.8360 - val_loss: 0.6306 - val_acc: 0.7835\n",
      "Epoch 131/150\n",
      "58/58 [==============================] - 16s 276ms/step - loss: 0.3966 - acc: 0.8408 - val_loss: 0.6072 - val_acc: 0.7892\n",
      "Epoch 132/150\n",
      "58/58 [==============================] - 15s 264ms/step - loss: 0.4018 - acc: 0.8383 - val_loss: 0.5605 - val_acc: 0.7942\n",
      "Epoch 133/150\n",
      "58/58 [==============================] - 16s 272ms/step - loss: 0.3877 - acc: 0.8425 - val_loss: 0.5014 - val_acc: 0.8190\n",
      "Epoch 134/150\n",
      "58/58 [==============================] - 16s 282ms/step - loss: 0.3954 - acc: 0.8401 - val_loss: 0.6129 - val_acc: 0.7888\n",
      "Epoch 135/150\n",
      "58/58 [==============================] - 15s 264ms/step - loss: 0.4108 - acc: 0.8321 - val_loss: 0.5877 - val_acc: 0.7998\n",
      "Epoch 136/150\n",
      "58/58 [==============================] - 17s 292ms/step - loss: 0.3848 - acc: 0.8488 - val_loss: 0.5193 - val_acc: 0.8004\n",
      "Epoch 137/150\n",
      "58/58 [==============================] - 17s 289ms/step - loss: 0.3857 - acc: 0.8395 - val_loss: 0.5186 - val_acc: 0.8091\n",
      "Epoch 138/150\n",
      "58/58 [==============================] - 18s 311ms/step - loss: 0.3596 - acc: 0.8485 - val_loss: 0.5466 - val_acc: 0.8047\n",
      "Epoch 139/150\n",
      "58/58 [==============================] - 17s 286ms/step - loss: 0.3999 - acc: 0.8363 - val_loss: 0.5988 - val_acc: 0.7867\n",
      "Epoch 140/150\n",
      "58/58 [==============================] - 15s 263ms/step - loss: 0.3889 - acc: 0.8416 - val_loss: 0.5501 - val_acc: 0.8029\n",
      "Epoch 141/150\n",
      "58/58 [==============================] - 16s 268ms/step - loss: 0.3898 - acc: 0.8379 - val_loss: 0.5462 - val_acc: 0.8165\n",
      "Epoch 142/150\n",
      "58/58 [==============================] - 16s 283ms/step - loss: 0.3747 - acc: 0.8400 - val_loss: 0.5345 - val_acc: 0.8212\n",
      "Epoch 143/150\n",
      "58/58 [==============================] - 16s 270ms/step - loss: 0.3859 - acc: 0.8418 - val_loss: 0.5148 - val_acc: 0.8197\n",
      "Epoch 144/150\n",
      "58/58 [==============================] - 15s 259ms/step - loss: 0.3971 - acc: 0.8454 - val_loss: 0.5887 - val_acc: 0.8078\n",
      "Epoch 145/150\n",
      "58/58 [==============================] - 15s 255ms/step - loss: 0.3909 - acc: 0.8408 - val_loss: 0.5791 - val_acc: 0.8078\n",
      "Epoch 146/150\n",
      "58/58 [==============================] - 14s 244ms/step - loss: 0.4045 - acc: 0.8327 - val_loss: 0.6039 - val_acc: 0.8018\n",
      "Epoch 147/150\n",
      "58/58 [==============================] - 14s 245ms/step - loss: 0.3917 - acc: 0.8437 - val_loss: 0.5440 - val_acc: 0.7935\n",
      "Epoch 148/150\n",
      "58/58 [==============================] - 15s 254ms/step - loss: 0.3912 - acc: 0.8368 - val_loss: 0.6412 - val_acc: 0.7842\n",
      "Epoch 149/150\n",
      "58/58 [==============================] - 16s 271ms/step - loss: 0.4078 - acc: 0.8378 - val_loss: 0.5854 - val_acc: 0.7960\n",
      "Epoch 150/150\n",
      "58/58 [==============================] - 16s 267ms/step - loss: 0.3688 - acc: 0.8478 - val_loss: 0.5848 - val_acc: 0.8112\n",
      "2406.57 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "start_time = time.time()\n",
    "history = model.fit_generator(train_batch, steps_per_epoch=len(x_train)//64, \n",
    "                       epochs=150,\n",
    "                    validation_data=test_batch, \n",
    "                    validation_steps=len(x_test)//64)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'{elapsed_time:.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "model.save('alpha_numeric.h5')\n",
    "index = 5\n",
    "prediction = model.predict(x_test)\n",
    "plt.imshow(x_test[index].reshape(32,32))\n",
    "print(prediction[index])\n",
    "print(np.argmax(prediction[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 143,667,240\n",
      "Trainable params: 143,667,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# VGG19\n",
    "from keras.applications.vgg19 import VGG19\n",
    "#convert 28x28 grayscale to 48x48 rgb channels\n",
    "vgg19_model = VGG19(include_top=False, input_shape=(32,32,3))\n",
    "vgg19_model2 = VGG19()\n",
    "vgg19_model2.summary()\n",
    "# vgg19_model.summary()\n",
    "# vgg19_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 61)                124989    \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 61)                0         \n",
      "=================================================================\n",
      "Total params: 30,641,277\n",
      "Trainable params: 10,616,893\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "our_model = Sequential()\n",
    "\n",
    "for layer in vgg19_model.layers:\n",
    "    our_model.add(layer)\n",
    "for layer in our_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "our_model.add(Flatten())\n",
    "our_model.add(Dropout(0.5))\n",
    "our_model.add(Dense(4096))\n",
    "our_model.add(Activation('relu'))\n",
    "\n",
    "our_model.add(Dropout(0.5))\n",
    "our_model.add(Dense(2048))\n",
    "our_model.add(Activation('relu'))\n",
    "our_model.add(Dropout(0.2))\n",
    "our_model.add(Dense(61))\n",
    "our_model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "our_model.summary()\n",
    "\n",
    "our_model.compile(loss='categorical_crossentropy', optimizer=Adam(), \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "29/29 [==============================] - 38s 1s/step - loss: 3.5491 - acc: 0.1623 - val_loss: 2.4381 - val_acc: 0.3682\n",
      "Epoch 2/150\n",
      "29/29 [==============================] - 37s 1s/step - loss: 2.2438 - acc: 0.3634 - val_loss: 1.7392 - val_acc: 0.5137\n",
      "Epoch 3/150\n",
      "29/29 [==============================] - 38s 1s/step - loss: 1.9008 - acc: 0.4414 - val_loss: 1.4402 - val_acc: 0.5933\n",
      "Epoch 4/150\n",
      "29/29 [==============================] - 37s 1s/step - loss: 1.6592 - acc: 0.4940 - val_loss: 1.3497 - val_acc: 0.6206\n",
      "Epoch 5/150\n",
      "29/29 [==============================] - 38s 1s/step - loss: 1.5494 - acc: 0.5262 - val_loss: 1.2464 - val_acc: 0.6393\n",
      "Epoch 6/150\n",
      "29/29 [==============================] - 40s 1s/step - loss: 1.4767 - acc: 0.5398 - val_loss: 1.2323 - val_acc: 0.6343\n",
      "Epoch 7/150\n",
      "29/29 [==============================] - 46s 2s/step - loss: 1.4243 - acc: 0.5568 - val_loss: 1.1187 - val_acc: 0.6468\n",
      "Epoch 8/150\n",
      "29/29 [==============================] - 55s 2s/step - loss: 1.3418 - acc: 0.5841 - val_loss: 1.0842 - val_acc: 0.6763\n",
      "Epoch 9/150\n",
      "29/29 [==============================] - 55s 2s/step - loss: 1.3289 - acc: 0.5823 - val_loss: 1.0532 - val_acc: 0.6716\n",
      "Epoch 10/150\n",
      " 6/29 [=====>........................] - ETA: 41s - loss: 1.2554 - acc: 0.5977"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-54ded58a7925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_steps=len(x_test)//128)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = our_model.fit_generator(train_batch, steps_per_epoch=len(x_train)\n",
    "                                                               //128, \n",
    "                       epochs=150,\n",
    "                    validation_data=test_batch, \n",
    "                    validation_steps=len(x_test)//128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot history\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "our_model.save('VGG19_Model.h5')\n",
    "result = our_model.evaluate_generator(test_batch, steps=len(x_test)//128)\n",
    "test_img, test_label = next(test_batch)\n",
    "test_label = test_label[:,0]\n",
    "\n",
    "predictions = our_model.predict_generator(test_batch, steps=1)\n",
    "\n",
    "# cm = confusion_matrix(test_label, np.round(predictions[:,0]))\n",
    "\n",
    "print(result)\n",
    "\n",
    "our_model.save('vgg19_model.h5')\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# MNIST model\n",
    "from keras.datasets import mnist\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "(x_train2, y_train2), (x_test2, y_test2) = mnist.load_data()\n",
    "x_train2 = x_train2.reshape(x_train2.shape[0], img_rows, img_cols, 1)\n",
    "x_test2 = x_test2.reshape(x_test2.shape[0], img_rows, img_cols, 1)\n",
    "input_shape2 = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train2 = x_train2.astype('float32')\n",
    "x_test2 = x_test2.astype('float32')\n",
    "x_train2 /= 255\n",
    "x_test2 /= 255\n",
    "print('x_train shape:', x_train2.shape)\n",
    "print(x_train2.shape[0], 'train samples')\n",
    "print(x_test2.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "y_train2 = keras.utils.to_categorical(y_train2, num_classes)\n",
    "y_test2 = keras.utils.to_categorical(y_test2, num_classes)\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape2))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "model2.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 96s 2ms/step - loss: 0.0871 - acc: 0.9745 - val_loss: 0.0420 - val_acc: 0.9860\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0652 - acc: 0.9808 - val_loss: 0.0351 - val_acc: 0.9888\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0530 - acc: 0.9838 - val_loss: 0.0343 - val_acc: 0.9888\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0463 - acc: 0.9862 - val_loss: 0.0305 - val_acc: 0.9899\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0399 - acc: 0.9879 - val_loss: 0.0295 - val_acc: 0.9906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c90bbfdd8>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train2, y_train2,\n",
    "           batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
